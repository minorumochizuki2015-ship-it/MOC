#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ„ãƒ¼ãƒ«
ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã®åˆ†æãƒ»å¯è¦–åŒ–
"""

import json
import statistics
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional

import matplotlib.pyplot as plt
import pandas as pd


class BenchmarkAnalyzer:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, data_dir: str = "data/logs/benchmarks"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

    def load_benchmark_data(self, days: int = 7) -> List[Dict[str, Any]]:
        """æŒ‡å®šæ—¥æ•°åˆ†ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        cutoff_date = datetime.now() - timedelta(days=days)
        all_data = []

        # æ—¥åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        for i in range(days):
            date = datetime.now() - timedelta(days=i)
            file_path = (
                self.data_dir / f"benchmark_data_{date.strftime('%Y%m%d')}.jsonl"
            )

            if file_path.exists():
                with open(file_path, "r", encoding="utf-8") as f:
                    for line in f:
                        try:
                            data = json.loads(line.strip())
                            # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                            if datetime.fromisoformat(data["timestamp"]) >= cutoff_date:
                                all_data.append(data)
                        except (json.JSONDecodeError, KeyError):
                            continue

        return all_data

    def analyze_performance_trends(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ"""
        if not data:
            return {"error": "No data available"}

        # ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã§ã‚½ãƒ¼ãƒˆ
        data.sort(key=lambda x: x["timestamp"])

        # åŸºæœ¬çµ±è¨ˆ
        durations = [d["duration"] for d in data if d.get("status") == "completed"]
        tokens_per_second = [
            d["tokens_per_second"] for d in data if d.get("status") == "completed"
        ]
        tokens_generated = [
            d["tokens_generated"] for d in data if d.get("status") == "completed"
        ]

        if not durations:
            return {"error": "No completed requests found"}

        # æ™‚ç³»åˆ—åˆ†æ
        hourly_stats = {}
        for item in data:
            if item.get("status") != "completed":
                continue

            hour = datetime.fromisoformat(item["timestamp"]).strftime("%Y-%m-%d %H:00")
            if hour not in hourly_stats:
                hourly_stats[hour] = {
                    "count": 0,
                    "durations": [],
                    "tokens_per_second": [],
                    "tokens_generated": [],
                }

            hourly_stats[hour]["count"] += 1
            hourly_stats[hour]["durations"].append(item["duration"])
            hourly_stats[hour]["tokens_per_second"].append(item["tokens_per_second"])
            hourly_stats[hour]["tokens_generated"].append(item["tokens_generated"])

        # æ™‚é–“åˆ¥å¹³å‡è¨ˆç®—
        hourly_averages = {}
        for hour, stats in hourly_stats.items():
            hourly_averages[hour] = {
                "avg_duration": statistics.mean(stats["durations"]),
                "avg_tokens_per_second": statistics.mean(stats["tokens_per_second"]),
                "avg_tokens_generated": statistics.mean(stats["tokens_generated"]),
                "request_count": stats["count"],
            }

        return {
            "total_requests": len(data),
            "completed_requests": len(durations),
            "error_rate": (len(data) - len(durations)) / len(data) if data else 0,
            "overall_stats": {
                "avg_duration": statistics.mean(durations),
                "median_duration": statistics.median(durations),
                "min_duration": min(durations),
                "max_duration": max(durations),
                "avg_tokens_per_second": statistics.mean(tokens_per_second),
                "median_tokens_per_second": statistics.median(tokens_per_second),
                "avg_tokens_generated": statistics.mean(tokens_generated),
                "total_tokens_generated": sum(tokens_generated),
            },
            "hourly_trends": hourly_averages,
        }

    def analyze_model_performance(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æ"""
        model_stats = {}

        for item in data:
            if item.get("status") != "completed":
                continue

            model_id = item.get("model_id", "unknown")
            if model_id not in model_stats:
                model_stats[model_id] = {
                    "count": 0,
                    "durations": [],
                    "tokens_per_second": [],
                    "tokens_generated": [],
                    "task_types": set(),
                }

            model_stats[model_id]["count"] += 1
            model_stats[model_id]["durations"].append(item["duration"])
            model_stats[model_id]["tokens_per_second"].append(item["tokens_per_second"])
            model_stats[model_id]["tokens_generated"].append(item["tokens_generated"])
            if item.get("task_type"):
                model_stats[model_id]["task_types"].add(item["task_type"])

        # çµ±è¨ˆè¨ˆç®—
        model_analysis = {}
        for model_id, stats in model_stats.items():
            model_analysis[model_id] = {
                "request_count": stats["count"],
                "avg_duration": statistics.mean(stats["durations"]),
                "avg_tokens_per_second": statistics.mean(stats["tokens_per_second"]),
                "avg_tokens_generated": statistics.mean(stats["tokens_generated"]),
                "task_types": list(stats["task_types"]),
                "efficiency_score": statistics.mean(stats["tokens_per_second"])
                / statistics.mean(stats["durations"]),
            }

        return model_analysis

    def generate_performance_report(self, days: int = 7) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        data = self.load_benchmark_data(days)

        if not data:
            return "ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

        # åˆ†æå®Ÿè¡Œ
        trends = self.analyze_performance_trends(data)
        model_perf = self.analyze_model_performance(data)

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = []
        report.append("=" * 60)
        report.append(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆ ({days}æ—¥é–“)")
        report.append("=" * 60)
        report.append("")

        # åŸºæœ¬çµ±è¨ˆ
        if "error" not in trends:
            overall = trends["overall_stats"]
            report.append("ğŸ“Š åŸºæœ¬çµ±è¨ˆ")
            report.append(f"  ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {trends['total_requests']}")
            report.append(f"  å®Œäº†ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {trends['completed_requests']}")
            report.append(f"  ã‚¨ãƒ©ãƒ¼ç‡: {trends['error_rate']:.2%}")
            report.append(f"  å¹³å‡å¿œç­”æ™‚é–“: {overall['avg_duration']:.3f}ç§’")
            report.append(f"  ä¸­å¤®å€¤å¿œç­”æ™‚é–“: {overall['median_duration']:.3f}ç§’")
            report.append(
                f"  å¹³å‡æ¨è«–é€Ÿåº¦: {overall['avg_tokens_per_second']:.2f} tok/s"
            )
            report.append(f"  ç·ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {overall['total_tokens_generated']:,}")
            report.append("")

        # ãƒ¢ãƒ‡ãƒ«åˆ¥åˆ†æ
        if model_perf:
            report.append("ğŸ¤– ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
            for model_id, stats in model_perf.items():
                report.append(f"  {model_id}:")
                report.append(f"    ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {stats['request_count']}")
                report.append(f"    å¹³å‡å¿œç­”æ™‚é–“: {stats['avg_duration']:.3f}ç§’")
                report.append(
                    f"    å¹³å‡æ¨è«–é€Ÿåº¦: {stats['avg_tokens_per_second']:.2f} tok/s"
                )
                report.append(f"    åŠ¹ç‡ã‚¹ã‚³ã‚¢: {stats['efficiency_score']:.2f}")
                report.append(f"    ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {', '.join(stats['task_types'])}")
                report.append("")

        # æ¨å¥¨äº‹é …
        report.append("ğŸ’¡ æ¨å¥¨äº‹é …")
        if "error" not in trends:
            avg_speed = trends["overall_stats"]["avg_tokens_per_second"]
            if avg_speed < 5:
                report.append(
                    "  - æ¨è«–é€Ÿåº¦ãŒé…ã„ã§ã™ã€‚GPUè¨­å®šã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
                )
            elif avg_speed < 10:
                report.append(
                    "  - æ¨è«–é€Ÿåº¦ã¯ä¸­ç¨‹åº¦ã§ã™ã€‚ã•ã‚‰ãªã‚‹æœ€é©åŒ–ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚"
                )
            else:
                report.append("  - æ¨è«–é€Ÿåº¦ã¯è‰¯å¥½ã§ã™ã€‚")

        if trends.get("error_rate", 0) > 0.1:
            report.append(
                "  - ã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ã„ã§ã™ã€‚ã‚µãƒ¼ãƒãƒ¼è¨­å®šã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        report.append(
            "  - å®šæœŸçš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç›£è¦–ã—ã¦ãã ã•ã„ã€‚"
        )
        report.append("")

        return "\n".join(report)

    def export_training_dataset(
        self, output_file: str = "data/training_dataset.jsonl"
    ) -> int:
        """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        data = self.load_benchmark_data(days=30)  # 30æ—¥åˆ†

        training_data = []
        for item in data:
            if item.get("status") == "completed" and not item.get("error"):
                training_data.append(
                    {
                        "prompt": item["prompt"],
                        "response": item["response"],
                        "model_id": item["model_id"],
                        "task_type": item.get("task_type"),
                        "performance_metrics": {
                            "duration": item["duration"],
                            "tokens_generated": item["tokens_generated"],
                            "tokens_per_second": item["tokens_per_second"],
                        },
                    }
                )

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            for item in training_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")

        return len(training_data)


# ä¾¿åˆ©é–¢æ•°
def generate_performance_report(days: int = 7) -> str:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ä¾¿åˆ©é–¢æ•°"""
    analyzer = BenchmarkAnalyzer()
    return analyzer.generate_performance_report(days)


def export_training_data(output_file: str = "data/training_dataset.jsonl") -> int:
    """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ä¾¿åˆ©é–¢æ•°"""
    analyzer = BenchmarkAnalyzer()
    return analyzer.export_training_dataset(output_file)
